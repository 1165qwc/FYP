{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial Expression Expressiveness Recognition System\n",
    "\n",
    "This notebook implements a CNN-based facial expression recognition system that classifies facial expressions into three categories based on expressiveness levels rather than traditional emotions:\n",
    "\n",
    "- **Reserved Expression**: Low facial expressiveness (negative facial expression scores)\n",
    "- **Balanced Expression**: Neutral facial expressiveness (facial expression scores around zero)\n",
    "- **Expressive**: High facial expressiveness (positive facial expression scores)\n",
    "\n",
    "This approach differs from Ekman's 7 basic emotions by focusing on the intensity of facial expressiveness rather than specific emotional states.\n",
    "\n",
    "The system uses:\n",
    "- **MediaPipe** for face detection (alternative to OpenCV)\n",
    "- **TensorFlow/Keras** for the CNN model (alternative to PyTorch)\n",
    "- **RecruitView_Data** dataset for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "# !pip install tensorflow mediapipe opencv-python pandas numpy scikit-learn matplotlib seaborn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata\n",
    "metadata_path = 'FYP/RecruitView_Data/metadata.jsonl'\n",
    "\n",
    "# Read the JSONL file\n",
    "data = []\n",
    "with open(metadata_path, 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nColumns:\", list(df.columns))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze facial expression scores distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['facial_expression'], bins=50, kde=True)\n",
    "plt.title('Distribution of Facial Expression Scores')\n",
    "plt.xlabel('Facial Expression Score')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(y=df['facial_expression'])\n",
    "plt.title('Box Plot of Facial Expression Scores')\n",
    "plt.ylabel('Facial Expression Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Facial Expression Score Statistics:\")\n",
    "print(df['facial_expression'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Design Expressiveness Classification System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define expressiveness categories based on facial expression scores\n",
    "def categorize_expressiveness(score):\n",
    "    \"\"\"\n",
    "    Categorize facial expression scores into expressiveness levels\n",
    "    \n",
    "    - Reserved Expression: score < -0.3 (low expressiveness)\n",
    "    - Balanced Expression: -0.3 <= score <= 0.3 (neutral expressiveness)\n",
    "    - Expressive: score > 0.3 (high expressiveness)\n",
    "    \"\"\"\n",
    "    if score < -0.3:\n",
    "        return 'Reserved Expression'\n",
    "    elif score <= 0.3:\n",
    "        return 'Balanced Expression'\n",
    "    else:\n",
    "        return 'Expressive'\n",
    "\n",
    "# Apply categorization\n",
    "df['expressiveness_category'] = df['facial_expression'].apply(categorize_expressiveness)\n",
    "\n",
    "# Display category distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "category_counts = df['expressiveness_category'].value_counts()\n",
    "sns.barplot(x=category_counts.index, y=category_counts.values)\n",
    "plt.title('Distribution of Expressiveness Categories')\n",
    "plt.xlabel('Expressiveness Category')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nExpressiveness Category Distribution:\")\n",
    "print(category_counts)\n",
    "print(f\"\\nTotal samples: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Face Detection and Feature Extraction Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Face Detection\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize face detection model\n",
    "face_detection = mp_face_detection.FaceDetection(\n",
    "    model_selection=1,  # Use full-range model for better accuracy\n",
    "    min_detection_confidence=0.5\n",
    ")\n",
    "\n",
    "print(\"MediaPipe Face Detection initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_face_from_frame(frame):\n",
    "    \"\"\"\n",
    "    Extract face from a video frame using MediaPipe\n",
    "    \n",
    "    Args:\n",
    "        frame: Video frame as numpy array\n",
    "    \n",
    "    Returns:\n",
    "        cropped_face: Cropped face image (48x48 grayscale) or None if no face detected\n",
    "    \"\"\"\n",
    "    # Convert BGR to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Process the frame\n",
    "    results = face_detection.process(rgb_frame)\n",
    "    \n",
    "    if results.detections:\n",
    "        # Get the first detected face\n",
    "        detection = results.detections[0]\n",
    "        \n",
    "        # Get bounding box\n",
    "        bbox = detection.location_data.relative_bounding_box\n",
    "        \n",
    "        # Convert relative coordinates to absolute\n",
    "        h, w, _ = frame.shape\n",
    "        x_min = int(bbox.xmin * w)\n",
    "        y_min = int(bbox.ymin * h)\n",
    "        width = int(bbox.width * w)\n",
    "        height = int(bbox.height * h)\n",
    "        \n",
    "        # Add some padding\n",
    "        padding = int(0.1 * width)\n",
    "        x_min = max(0, x_min - padding)\n",
    "        y_min = max(0, y_min - padding)\n",
    "        x_max = min(w, x_min + width + 2*padding)\n",
    "        y_max = min(h, y_min + height + 2*padding)\n",
    "        \n",
    "        # Crop face\n",
    "        face_crop = frame[y_min:y_max, x_min:x_max]\n",
    "        \n",
    "        if face_crop.size > 0:\n",
    "            # Convert to grayscale\n",
    "            face_gray = cv2.cvtColor(face_crop, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # Resize to 48x48\n",
    "            face_resized = cv2.resize(face_gray, (48, 48))\n",
    "            \n",
    "            return face_resized\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"Face extraction function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Video Frame Extraction and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames_from_video(video_path, max_frames=10, frame_skip=5):\n",
    "    \"\"\"\n",
    "    Extract frames from video for facial expression analysis\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to video file\n",
    "        max_frames: Maximum number of frames to extract\n",
    "        frame_skip: Skip every N frames to get diverse expressions\n",
    "    \n",
    "    Returns:\n",
    "        faces: List of extracted face images\n",
    "    \"\"\"\n",
    "    faces = []\n",
    "    \n",
    "    try:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        \n",
    "        if not cap.isOpened():\n",
    "            print(f\"Could not open video: {video_path}\")\n",
    "            return faces\n",
    "        \n",
    "        frame_count = 0\n",
    "        extracted_count = 0\n",
    "        \n",
    "        while extracted_count < max_frames:\n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Skip frames for diversity\n",
    "            if frame_count % frame_skip == 0:\n",
    "                face = extract_face_from_frame(frame)\n",
    "                if face is not None:\n",
    "                    faces.append(face)\n",
    "                    extracted_count += 1\n",
    "            \n",
    "            frame_count += 1\n",
    "            \n",
    "            # Safety check to prevent infinite loops\n",
    "            if frame_count > 1000:\n",
    "                break\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing video {video_path}: {str(e)}\")\n",
    "    \n",
    "    return faces\n",
    "\n",
    "print(\"Video frame extraction function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset by extracting faces from videos\n",
    "# Note: This is a demonstration - in practice, you might want to limit the number of videos processed\n",
    "\n",
    "# For demonstration, let's process a subset of videos\n",
    "sample_df = df.head(50)  # Process first 50 videos for demonstration\n",
    "\n",
    "X_faces = []\n",
    "y_labels = []\n",
    "\n",
    "print(\"Extracting faces from videos...\")\n",
    "for idx, row in tqdm(sample_df.iterrows(), total=len(sample_df)):\n",
    "    video_path = f\"FYP/RecruitView_Data/{row['file_name']}\"\n",
    "    \n",
    "    if os.path.exists(video_path):\n",
    "        faces = extract_frames_from_video(video_path, max_frames=3)\n",
    "        \n",
    "        for face in faces:\n",
    "            X_faces.append(face)\n",
    "            y_labels.append(row['expressiveness_category'])\n",
    "    else:\n",
    "        print(f\"Video not found: {video_path}\")\n",
    "\n",
    "print(f\"\\nExtracted {len(X_faces)} face images from {len(sample_df)} videos\")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "if X_faces:\n",
    "    X = np.array(X_faces)\n",
    "    X = X.reshape(-1, 48, 48, 1)  # Add channel dimension\n",
    "    X = X.astype('float32') / 255.0  # Normalize\n",
    "    \n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y_labels)\n",
    "    y = keras.utils.to_categorical(y_encoded, num_classes=3)\n",
    "    \n",
    "    print(f\"X shape: {X.shape}\")\n",
    "    print(f\"y shape: {y.shape}\")\n",
    "    print(f\"Classes: {label_encoder.classes_}\")\n",
    "else:\n",
    "    print(\"No faces extracted. Please check video paths and face detection.\")\n",
    "    X = None\n",
    "    y = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build CNN Model with TensorFlow/Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_expressiveness_model():\n",
    "    \"\"\"\n",
    "    Create CNN model for facial expressiveness recognition\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        # Input layer\n",
    "        keras.layers.Input(shape=(48, 48, 1)),\n",
    "        \n",
    "        # First convolutional block\n",
    "        keras.layers.Conv2D(32, (3, 3), padding='same'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Activation('relu'),\n",
    "        keras.layers.MaxPooling2D((2, 2)),\n",
    "        keras.layers.Dropout(0.25),\n",
    "        \n",
    "        # Second convolutional block\n",
    "        keras.layers.Conv2D(64, (3, 3), padding='same'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Activation('relu'),\n",
    "        keras.layers.MaxPooling2D((2, 2)),\n",
    "        keras.layers.Dropout(0.25),\n",
    "        \n",
    "        # Third convolutional block\n",
    "        keras.layers.Conv2D(128, (3, 3), padding='same'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Activation('relu'),\n",
    "        keras.layers.MaxPooling2D((2, 2)),\n",
    "        keras.layers.Dropout(0.25),\n",
    "        \n",
    "        # Flatten and dense layers\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(256, activation='relu'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        \n",
    "        # Output layer\n",
    "        keras.layers.Dense(3, activation='softmax')  # 3 classes: Reserved, Balanced, Expressive\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "model = create_expressiveness_model()\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Display model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if X is not None and len(X) > 0:\n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y_encoded\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set shape: {X_train.shape}\")\n",
    "    print(f\"Test set shape: {X_test.shape}\")\n",
    "    \n",
    "    # Data augmentation\n",
    "    datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "        rotation_range=10,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        horizontal_flip=True,\n",
    "        zoom_range=0.1\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        datagen.flow(X_train, y_train, batch_size=32),\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=50,\n",
    "        callbacks=[\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor='val_accuracy',\n",
    "                patience=10,\n",
    "                restore_best_weights=True\n",
    "            ),\n",
    "            keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=5,\n",
    "                min_lr=1e-6\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTraining completed!\")\n",
    "else:\n",
    "    print(\"No data available for training. Please check the data extraction process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'history' in locals():\n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"\\nTest Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true_classes, y_pred_classes, \n",
    "                               target_names=label_encoder.classes_))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=label_encoder.classes_,\n",
    "                yticklabels=label_encoder.classes_)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No training history available. Model was not trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Save the model\n",
    "model.save('models/Facial_Expressiveness_Recognition_Model.h5')\n",
    "\n",
    "# Save model architecture as JSON\n",
    "model_json = model.to_json()\n",
    "with open('models/Facial_Expressiveness_Recognition_Model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# Save weights\n",
    "model.save_weights('models/expressiveness_model_weights.h5')\n",
    "\n",
    "# Save label encoder classes\n",
    "np.save('models/label_encoder_classes.npy', label_encoder.classes_)\n",
    "\n",
    "print(\"Model saved successfully!\")\n",
    "print(\"Files saved:\")\n",
    "print(\"- models/Facial_Expressiveness_Recognition_Model.h5\")\n",
    "print(\"- models/Facial_Expressiveness_Recognition_Model.json\")\n",
    "print(\"- models/expressiveness_model_weights.h5\")\n",
    "print(\"- models/label_encoder_classes.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Real-time Testing (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-time facial expressiveness recognition\n",
    "# Uncomment and run this cell to test with webcam\n",
    "\n",
    "\"\"\"\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# Load the model\n",
    "model = model_from_json(open(\"models/Facial_Expressiveness_Recognition_Model.json\", \"r\").read())\n",
    "model.load_weights('models/expressiveness_model_weights.h5')\n",
    "\n",
    "# Load label encoder classes\n",
    "label_classes = np.load('models/label_encoder_classes.npy')\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Extract face\n",
    "    face = extract_face_from_frame(frame)\n",
    "    \n",
    "    if face is not None:\n",
    "        # Preprocess face\n",
    "        face_input = face.reshape(1, 48, 48, 1).astype('float32') / 255.0\n",
    "        \n",
    "        # Predict expressiveness\n",
    "        prediction = model.predict(face_input, verbose=0)\n",
    "        predicted_class = np.argmax(prediction)\n",
    "        expressiveness_label = label_classes[predicted_class]\n",
    "        confidence = prediction[0][predicted_class]\n",
    "        \n",
    "        # Display result on frame\n",
    "        cv2.putText(frame, f'{expressiveness_label}: {confidence:.2f}', \n",
    "                   (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "    # Display frame\n",
    "    cv2.imshow('Facial Expressiveness Recognition', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implements a facial expression recognition system that focuses on **expressiveness levels** rather than traditional emotions:\n",
    "\n",
    "### Key Features:\n",
    "1. **Alternative to Ekman's 7 emotions**: Classifies into Reserved, Balanced, and Expressive categories\n",
    "2. **MediaPipe instead of OpenCV**: Uses Google's MediaPipe for face detection\n",
    "3. **TensorFlow/Keras instead of PyTorch**: Uses TensorFlow/Keras for the CNN model\n",
    "4. **RecruitView_Data dataset**: Uses the provided interview video dataset\n",
    "\n",
    "### Model Architecture:\n",
    "- 3 Convolutional layers with batch normalization and dropout\n",
    "- Dense layer with 256 neurons\n",
    "- Output layer with 3 classes (softmax)\n",
    "\n",
    "### Categories:\n",
    "- **Reserved Expression**: Low expressiveness (facial expression score < -0.3)\n",
    "- **Balanced Expression**: Neutral expressiveness (-0.3 ≤ score ≤ 0.3)\n",
    "- **Expressive**: High expressiveness (score > 0.3)\n",
    "\n",
    "### Usage:\n",
    "The model can be used to analyze facial expressiveness in interview videos or real-time webcam feed, providing insights into candidates' communication styles during interviews."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/plain",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}