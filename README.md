# Facial Expression Expressiveness Recognition System

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.17+-orange.svg)](https://tensorflow.org/)
[![MediaPipe](https://img.shields.io/badge/MediaPipe-0.10+-green.svg)](https://mediapipe.dev/)

A novel facial expression recognition system that classifies **expressiveness levels** rather than traditional emotions. Perfect for interview analysis and communication research.

## ğŸ¯ What's Different

| Feature | Traditional Emotion Recognition | This System |
|---------|-------------------------------|-------------|
| **Categories** | 7 emotions (anger, fear, happy, etc.) | **3 expressiveness levels** |
| **Face Detection** | OpenCV Haar cascades | **MediaPipe** (Google's framework) |
| **Deep Learning** | PyTorch + TorchVision | **TensorFlow/Keras** |
| **Dataset** | FER2013 (general faces) | **RecruitView_Data** (interview videos) |
| **Use Case** | General emotion detection | **Interview expressiveness analysis** |

## ğŸš€ Quick Start

### 1. Clone & Setup Environment
```bash
# Clone repository
git clone https://github.com/yourusername/facial-expression-expressiveness.git
cd facial-expression-expressiveness

# Create virtual environment
python -m venv facial_expressiveness_env
facial_expressiveness_env\Scripts\activate  # Windows
# source facial_expressiveness_env/bin/activate  # Linux/Mac

# Install dependencies
pip install -r requirements.txt
```

### 2. Test the System
```bash
# Test if everything works
python test_system.py
```

### 3. Train Your Model
```bash
# Open Jupyter notebook
jupyter notebook Facial_Expression_Expressiveness_Recognition.ipynb

# Run all cells to train the model
```

### 4. Run Real-Time Recognition
```bash
# Start webcam recognition
python real_time_demo.py
```

## ğŸ“Š Expressiveness Categories

The system classifies facial expressions into three levels based on statistical analysis:

- **ğŸ˜ Reserved Expression**: Low facial expressiveness (score < -0.303)
- **ğŸ™‚ Balanced Expression**: Neutral expressiveness (-0.303 â‰¤ score â‰¤ 0.294)
- **ğŸ˜Š Expressive**: High expressiveness (score > 0.294)

## ğŸ—ï¸ Project Structure

```
facial-expression-expressiveness/
â”œâ”€â”€ ğŸ““ Facial_Expression_Expressiveness_Recognition.ipynb  # Main training notebook
â”œâ”€â”€ ğŸ¥ real_time_demo.py                                   # Real-time webcam demo
â”œâ”€â”€ ğŸ§ª test_system.py                                      # System testing
â”œâ”€â”€ ğŸ“‹ requirements.txt                                    # Python dependencies
â”œâ”€â”€ ğŸ“– README.md                                            # This file
â”œâ”€â”€ ğŸ“š USAGE_GUIDE.md                                       # Detailed documentation
â”œâ”€â”€ ğŸ” analyze_facial_data.py                              # Data analysis tools
â”œâ”€â”€ ğŸ“Š quick_analysis.py                                   # Fast data analysis
â””â”€â”€ ğŸ“ FYP/RecruitView_Data/                               # Interview dataset
    â”œâ”€â”€ ğŸ“„ metadata.jsonl                                  # Interview metadata
    â””â”€â”€ ğŸ¬ videos/                                         # Interview video files
```

## ğŸ¥ Real-Time Demo

The system provides live facial expressiveness recognition via webcam:

```bash
python real_time_demo.py
```

**Features:**
- âœ… Real-time face detection using MediaPipe
- âœ… Live expressiveness classification
- âœ… Visual feedback with bounding boxes
- âœ… Confidence scores display
- âœ… Press 'q' to quit

**Sample Output:**
```
Balanced Expression: 0.87
```

## ğŸ¤– Training Process

### Data Pipeline
1. **Load Interview Data**: Parse metadata from 2011 interview records
2. **Face Extraction**: Extract faces from video frames using MediaPipe
3. **Category Classification**: Label faces based on expressiveness scores
4. **Data Augmentation**: Apply rotations, shifts, and flips for robustness

### Model Architecture
```
Input (48x48 grayscale)
    â†“
Conv2D (32 filters) â†’ BatchNorm â†’ ReLU â†’ MaxPool â†’ Dropout (0.25)
    â†“
Conv2D (64 filters) â†’ BatchNorm â†’ ReLU â†’ MaxPool â†’ Dropout (0.25)
    â†“
Conv2D (128 filters) â†’ BatchNorm â†’ ReLU â†’ MaxPool â†’ Dropout (0.25)
    â†“
Flatten â†’ Dense (256) â†’ BatchNorm â†’ Dropout (0.5) â†’ Dense (3) â†’ Softmax
```

### Training Configuration
- **Optimizer**: Adam (lr=0.001)
- **Loss**: Categorical Crossentropy
- **Batch Size**: 32
- **Epochs**: Up to 50 (with early stopping)
- **Data Augmentation**: Rotation, shift, flip, zoom

## ğŸ“ˆ Performance

**Expected Results:**
- **Training Time**: 10-30 minutes
- **Test Accuracy**: 75-85%
- **Real-time FPS**: 15-30 FPS (CPU)

## ğŸ”§ Requirements

- **Python**: 3.8 or higher
- **RAM**: 4GB minimum, 8GB recommended
- **Storage**: 2GB for models and data
- **Camera**: Webcam for real-time demo
- **GPU**: Optional (TensorFlow uses CPU by default)

## ğŸ“¦ Dependencies

```
tensorflow==2.16.1          # Deep learning framework
keras==3.0.5                # High-level neural networks API
mediapipe==0.10.11          # Face detection and tracking
opencv-python==4.9.0.80     # Computer vision
numpy==1.26.4               # Numerical computing
pandas==2.2.2               # Data manipulation
matplotlib==3.8.4           # Plotting
seaborn==0.13.2             # Statistical visualization
scikit-learn==1.4.2         # Machine learning utilities
tqdm==4.66.4                # Progress bars
jupyter==1.0.0              # Interactive notebooks
```

## ğŸ¯ Use Cases

- **ğŸ¤ Interview Analysis**: Evaluate candidate expressiveness during video interviews
- **ğŸ“š Communication Research**: Study facial expressiveness patterns
- **ğŸ¢ HR Applications**: Assess non-verbal communication skills
- **ğŸ¤ Behavioral Analysis**: Understand communication styles

## ğŸ” Key Advantages

âœ… **Domain-Specific**: Trained on interview data instead of general faces
âœ… **Expressiveness Focus**: Measures communication style rather than basic emotions
âœ… **Modern Tech Stack**: Uses cutting-edge MediaPipe and TensorFlow
âœ… **Balanced Categories**: Uses statistical percentiles for fair distribution
âœ… **No Overlap**: Completely different approach from traditional emotion recognition

## ğŸš¨ Troubleshooting

### Model Not Found
```bash
# Train the model first
jupyter notebook Facial_Expression_Expressiveness_Recognition.ipynb
# Run all cells to create models/ directory
```

### Import Errors
```bash
# Reinstall dependencies
pip install --upgrade -r requirements.txt
```

### Webcam Issues
```bash
# Check camera permissions
# Close other camera applications
python test_system.py  # Test camera access
```

### Memory Issues
```bash
# Reduce batch size in notebook
# Process fewer videos during training
# Close other applications
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **RecruitView_Data**: Interview dataset for training
- **MediaPipe**: Google's face detection framework
- **TensorFlow**: Deep learning framework
- **Original Inspiration**: live-face-emotion-classifier project

## ğŸ“ Contact

For questions or suggestions, please open an issue on GitHub.

---

**Ready to analyze facial expressiveness?** ğŸš€

```bash
python real_time_demo.py
```